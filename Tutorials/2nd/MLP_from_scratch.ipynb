{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8039124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f54d9",
   "metadata": {},
   "source": [
    "## Load data (no split because we JUST want to overfit now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d59e7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data\n",
    "print(X.shape)\n",
    "\n",
    "Y = boston.target\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e8840",
   "metadata": {},
   "source": [
    "## Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb0b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "Y = (Y - Y.mean()) / Y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b075b6",
   "metadata": {},
   "source": [
    "## Sigmoid (naive) activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45bce7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd3ccec",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8864d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C = X.shape\n",
    "assert(N == Y.shape[0])\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 0.001 # Learning rate\n",
    "H = 64 # Dimension hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004abb43",
   "metadata": {},
   "source": [
    "## Weights to be learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e228d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.uniform(size=(C, H))\n",
    "W2 = np.random.uniform(size=(H, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd54ac",
   "metadata": {},
   "source": [
    "### Training process\n",
    "\n",
    "#### 1) Feed input data to first layer neural network\n",
    "\n",
    "$ Y1 = X x W_1 $\n",
    "\n",
    "#### 2) Apply sigmoid activation function\n",
    "\n",
    "$ Y2 = \\sigma(Y1) $\n",
    "\n",
    "#### 3) Second layer neural network\n",
    "\n",
    "$ Y3 = Y2 x W_2 $\n",
    "\n",
    "#### 4) Compute error / loss function\n",
    "$ E = \\frac{1}{N} \\sum_{i=1}^{N} (Y3_i- Y_i)^{2} $\n",
    "\n",
    "#### 5) Calculate partial derivatives of the Error wrt W1 and W2 using chain rule\n",
    "\n",
    "#### 6) Update weights using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19180717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.20931457773895\n",
      "55.92929333397451\n",
      "15.559100093507626\n",
      "4.883764823022697\n",
      "2.0477019275198884\n",
      "1.287347462546851\n",
      "1.0762838047313494\n",
      "1.0102410340565628\n",
      "0.9824229558253965\n",
      "0.9647765335238451\n",
      "0.9500097466349399\n",
      "0.9362602949484958\n",
      "0.9230637490287403\n",
      "0.9103045769363491\n",
      "0.8979489814883994\n",
      "0.8859797821058979\n",
      "0.8743820515347\n",
      "0.8631407511763487\n",
      "0.8522408059924096\n",
      "0.8416674532205708\n",
      "0.8314064777576087\n",
      "0.8214443257774195\n",
      "0.8117681415725201\n",
      "0.8023657631229764\n",
      "0.7932256975170752\n",
      "0.7843370875941462\n",
      "0.775689675624121\n",
      "0.7672737668847112\n",
      "0.7590801944788688\n",
      "0.7511002859705829\n",
      "0.7433258320386777\n",
      "0.735749057165177\n",
      "0.7283625922896204\n",
      "0.7211594493241882\n",
      "0.7141329974118513\n",
      "0.7072769408086471\n",
      "0.7005852982753461\n",
      "0.6940523838702434\n",
      "0.6876727890420035\n",
      "0.6814413659287413\n",
      "0.6753532117764964\n",
      "0.6694036543968003\n",
      "0.6635882385891205\n",
      "0.6579027134595923\n",
      "0.65234302057262\n",
      "0.6469052828767059\n",
      "0.6415857943502463\n",
      "0.6363810103170722\n",
      "0.6312875383852312\n",
      "0.626302129965929\n",
      "0.6214216723327017\n",
      "0.616643181183802\n",
      "0.6119637936734642\n",
      "0.6073807618801905\n",
      "0.6028914466824871\n",
      "0.5984933120145954\n",
      "0.5941839194767153\n",
      "0.5899609232760231\n",
      "0.5858220654764629\n",
      "0.5817651715368357\n",
      "0.5777881461181429\n",
      "0.5738889691424687\n",
      "0.5700656920869208\n",
      "0.5663164344972822\n",
      "0.5626393807070919\n",
      "0.5590327767488479\n",
      "0.5554949274449439\n",
      "0.5520241936667869\n",
      "0.5486189897513385\n",
      "0.5452777810650449\n",
      "0.5419990817058005\n",
      "0.5387814523342196\n",
      "0.5356234981260783\n",
      "0.5325238668383288\n",
      "0.5294812469815977\n",
      "0.5264943660925516\n",
      "0.5235619890999488\n",
      "0.5206829167786058\n",
      "0.5178559842858887\n",
      "0.5150800597756884\n",
      "0.512354043085173\n",
      "0.5096768644899172\n",
      "0.5070474835232893\n",
      "0.5044648878562504\n",
      "0.5019280922339633\n",
      "0.4994361374658427\n",
      "0.49698808946589446\n",
      "0.4945830383403921\n",
      "0.49222009752012624\n",
      "0.4898984029346425\n",
      "0.4876171122260391\n",
      "0.4853754040000573\n",
      "0.4831724771123336\n",
      "0.4810075499878194\n",
      "0.4788798599714972\n",
      "0.47678866270863884\n",
      "0.47473323155296043\n",
      "0.47271285700112875\n",
      "0.47072684615217186\n",
      "0.4687745221904306\n"
     ]
    }
   ],
   "source": [
    "for n_epoch in range(epochs):\n",
    "    \n",
    "    epoch_losses = []\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        x = X[i:i+batch_size]\n",
    "        y = np.expand_dims(Y[i:i+batch_size], -1)\n",
    "        assert(len(x) == len(y))\n",
    "        \n",
    "        # Forward path\n",
    "        Y1 = np.dot(x, W1)\n",
    "        Y2 = sigmoid(Y1)\n",
    "        Y3 = np.dot(Y2, W2)\n",
    "        #print(Y1.shape, Y2.shape, Y3.shape)\n",
    "\n",
    "        # Error calculation\n",
    "        E = np.mean((y - Y3)**2)\n",
    "        epoch_losses.append(E)\n",
    "        \n",
    "        # Backward path        \n",
    "        dE_dY3 = - (2 / len(y)) * (y - Y3)\n",
    "        dE_dW2 = np.dot(dE_dY3.T, Y2).T\n",
    "\n",
    "        dE_dY2 = np.dot(dE_dY3, W2.T)\n",
    "        dE_dY1 = dE_dY2 * derivative_sigmoid(Y2)\n",
    "        dE_dW1 = np.dot(dE_dY1.T, x).T\n",
    "        \n",
    "        # Gradient descent updates\n",
    "        W2 = W2 - lr * dE_dW2\n",
    "        W1 = W1 - lr * dE_dW1\n",
    "    \n",
    "    print(np.mean(np.array(epoch_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242de580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
